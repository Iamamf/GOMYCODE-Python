{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "\n",
    "def get_html_content(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Fetch and parse the HTML content of a Wikipedia page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an error for HTTP errors\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def extract_title(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract and return the article title from the HTML content.\"\"\"\n",
    "    title_tag = soup.find('h1', id='firstHeading')\n",
    "    return title_tag.text if title_tag else \"Title not found\"\n",
    "\n",
    "def extract_text_and_headings(soup: BeautifulSoup) -> Dict[str, List[str]]:\n",
    "    \"\"\"Extract headings and associated paragraph texts from the article.\"\"\"\n",
    "    content = {}\n",
    "    current_heading = None\n",
    "    \n",
    "    for element in soup.find(id=\"bodyContent\").find_all(['h2', 'p']):\n",
    "        if element.name == 'h2':\n",
    "            heading = element.get_text().strip()\n",
    "            current_heading = heading\n",
    "            content[current_heading] = []\n",
    "        elif element.name == 'p' and current_heading:\n",
    "            paragraph = element.get_text().strip()\n",
    "            content[current_heading].append(paragraph)\n",
    "            \n",
    "    return content\n",
    "\n",
    "def extract_links(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Collect all links that redirect to other Wikipedia pages.\"\"\"\n",
    "    links = []\n",
    "    for link in soup.find(id=\"bodyContent\").find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith('/wiki/') and not ':' in href:\n",
    "            links.append(f\"https://en.wikipedia.org{href}\")\n",
    "    return list(set(links))\n",
    "\n",
    "def scrape_wikipedia(url: str) -> Dict[str, any]:\n",
    "    \"\"\"Wrapper function to scrape the title, text with headings, and links from a Wikipedia page.\"\"\"\n",
    "    soup = get_html_content(url)\n",
    "    \n",
    "    title = extract_title(soup)\n",
    "    content = extract_text_and_headings(soup)\n",
    "    links = extract_links(soup)\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"links\": links\n",
    "    }\n",
    "\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "data = scrape_wikipedia(url)\n",
    "print(data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
